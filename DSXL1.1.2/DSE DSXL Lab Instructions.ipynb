{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# DSX Local Hands On Lab - Version 1.0.5\n\n## IBM Data Science Elite Team \n\nIn this hands-on lab excercise, you will be playing the role of a data scientist at Cognitive Bank. You will use IBM Data Science Experience Local to work through a data science and machine learning use case.  In this use case you will explore the full end-to-end machine learning process and the range of advanced functionality provided by  IBM DSX Local such as:\n* Support for multiple data sources and data formats\n* Integration with other environments such as Hortonworks Data Platform\n* Open platform environment with support for multiple languages and GUI tools\n* Collaboration across projects and teams, integration with Git repositories  \n* Deployment and model management capabilities to operationalize machine learning\n\nIn this scenario, your company's historical data are stored in HDFS within a Hortonworks Data Platform (HDP) Hadoop cluster.  You will first review the company's data assets using the HDP tooling, then you will create a data source connection from DSX Local to the HDP HDFS cluster.  Then, using DSX Local, you will explore historical data related to customer churn using a Jupyter notebook and the Python language (*customer \"churn\" is when a customer stops doing business with your company*). \n\nAfter visualizing and exploring historical data, you will then develop and test a machine learning model that will predict a given customer's probability to churn.   You will build two machine learning models, one built using a programmatic approach, the other built using a wizard called the Model Builder.  In the programmatic approach you will use the Scala programming language to train, evaluate, publish, deploy, and test a machine learning model all from within a Jupyter notebook.  Then, using the DSX Local Model Builder, you will train, publish, deploy and test a similar machine learning model without writing any code.  After developing and deploying these models, you will use the model management tools in DSX Local to setup ongoing evaluation of model performance.  These lab exercises will take approximately 1-1.5 hours to complete.\n\n### This document is also available at the following links:\n1. [Shared DSX Cloud notebook](https://ibm.co/2mwoMK6)\n2. [Shared DSX Cloud notebook, alternate link](https://dataplatform.ibm.com/analytics/notebooks/v2/61cd7b67-0776-401c-b859-a7f8fb049b05/view?access_token=dc3ec7063543479f110c6801421c35a82cabe8a1cb63dcd1840705d33d168874)\n3. [Github](https://github.com/mwalli/DSXLAB)", 
            "cell_type": "markdown", 
            "metadata": {
                "slideshow": {
                    "slide_type": "-"
                }
            }, 
            "attachments": {}
        }, 
        {
            "source": "# 1. Preparation steps\n## 1.1. __Lab Environment Overview. __\nThe systems you'll be using for this lab are hosted in a cloud environment called Skytap.  This skytap environment contains both a DSX Local system and a Hortonworks Data Platform environment including the Ambari systems management tool.  Your team will be assigned to an environment that will also be shared with other teams in the hands on lab.  Use only the login ID provided to your team and please pay close attention to how you name projects, models, and other resources so you will be able easily identify the assets created by your team.\n\n>__Required Web browser__:  Use either the __Chrome or Firefox__ browser on your personal workstation to complete these lab excercises.\n\n> __Certificate warnings__:  When you initially connect to the DSX Local application, depending on whether you are using Firefox or Chrome you may receive an \"insecure connection\" or \"connection not private\" warning due to the untrusted certificate that was used during the DSX Local installation.   You can safely ignore these warnings and proceed to the site (Firefox users click \"Advanced\" and click \"Add Exception\", Chrome users click \"Advanced\" and click \"Proceed to URL\" \n\n<BR><BR>\n\n## 1.2. __ Make note of the URL and credentials provided to your team for accessing the the DSX Local and Ambari system__\nThe lab instructors will provide you with the following information.  You will need the following:\n\n1. **DSX Local URL:**\n\n2. **DSX Local username/password:**\n\n3. **Ambari URL:**\n\n4. **Ambari username/password:**\n\n> **NOTE:**  **Use only the URLs and login information provided to your team by the lab instructors.**  These lab excercises will not function properly if multiple teams use the same URL and login information.  Therefore, use only the URLs or login information provided to you by the lab instructors and and please do not share your URLs and login information with other teams. **Please follow lab procedures exactly as documented.**  If you need assistance with the procedures or if you encounter problems or errors with the system, please inform a lab instructor and they will help address the issue.\n\n<BR><BR><BR><BR><BR><BR><BR><BR>\n\n## 1.2. Review data assets in Ambari\nAmbari is a management tool within the Hortonworks Data Platform environment.  In this step, you will use the Ambari file browser utility to familiarize yourself with your company's historical customer churn data contained within HDP HDFS:\n\n1. Login to Ambari using the credentials you noted above.\n2. Navigate to the \"Files view\" tool  (*Find this tool up in the toolbar within the \"Views\" dropdown menu to the right of the \"Admin\" menu, the icon looks like 9 squares*)\n3. Navigate to /data/FinancialServices/Churn/\n4. Verify you see the assets listed in the table below in the file browser.  In a following step you will create data assets in DSX that correspond to these files.\n\n| __Filename in HDP__ (in /data/FinancialServices/Churn/) | __Used For__                               | __Data Description__                                                               |\n|---------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------------------------------|\n| __churn_rate_visualization.csv__                        | Python Notebook: *Churn Visualization Python HDP*          | Churn rates for Python visualization notebook                                                   |\n| __cust_summary_visualization.csv__                      | Python Notebook:  *Churn Visualization Python HDP*         | Summarized customer data for Python visualization notebook    |\n| __cust_summary_notebook_training.csv__                  | Scala Notebook:  *Churn ML Training Notebook Scala HDP LR* | Customer churn data for programmatic model training from within a Scala notebook |\n| __cust_summary_visbuilder_training.csv__                | Model Builder: *Training ML model*                           | Summarized customer churn data for training model with the model builder    |\n\n<BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR>\n    \n## 1.3. Download copies of required Jupyter notebooks to your personal system\nIn this step you will open an existing project in DSX Local that is shared with your user ID.  You will then open and download Jupyter notebooks to your local system so you can reuse them in the following steps.\n1. Login to DSX Local as your assigned team user using the URL and credentials provided to you by the lab instructor.  \n2. In the \"Recently updated projects\" list, you will see a shared project called \"BankChurn\".  *Note:  Do not perform your lab excercises in this project, you only have \"viewer\" priveleges for this project.  You will open, then download the notebooks from this project.*\n3. Click into the BankChurn project, then click \"Assets\".  You should see a list of two notebooks contained in the project.\n4. Download both the __Churn Visualization Python HDP__ and __Churn ML Training Notebook Scala HDP LR__ notebooks.  \nTo do this, first open the notebook by clicking its link, then download it using the notebook's `File -> Download as -> Notebook (.ipynb)` menu option.  *Be patient:  Jupyter may take 15-30 seconds to load the notebook*\n> *Note: Remember the download location for these files, you will need these files in a following step.*\n5. After downloading both .ipynb files, exit the Jupyter notebook by clicking on the \"BankChurn\" project name in the link bar near the top left of the screen.  \n6. To conserve system resources, stop the kernel that was launched when you opened each notebook (shown as a green circles) by selecting \"Stop Kernel\" from the 3-dot menu to the right of the notebook name in the assets list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 2. Create a new DSX Local project, create a data source connection to HDP and create the associated data assets\nIn DSX, all work is performed within one or more projects.  In this step you will create a new project, connect to a data source, and add datasets to your project.  \n\n## 2.1 Create a new DSX project\n1. Return to your \"All Projects\" list either by clicking \"View all Projects\" from the the 3 horizontal bar menu at the top-left corner of the screen, or by clicking \"Projects\" in the navigation link area of the screen.\n2. Click \"New Project\" and in the \"Name\" field enter \"__[TeamLetter]__Team_BankChurnLab\" (e.g. ATeam_BankChurnLab, BTeam_BankChurnLab, etc), then click the Create button.\n>Note: As you progress through the lab excercises, you will see an asterisk \\* next to the project's name and the message *Changes made -- \nYou have local changes that you can commit* will be displayed. DSXL internally uses git  to manage project changes.  You can commit changes if you would like to but it is not necessary for the excercises in this lab.  If you did have collaborators added to the project, they would not see any additions or changes made to the project until you commit your changes to the project.  \n\n## 2.2 Create a new data source connection to HDP and add associated data sets to the project.\nYour DSX Local system is already configured to connect securely to the HDP HDFS system using the Knox protocol, but each project must contain a \"Data source\" with the correct connection information for HDFS.  Once the data source is created, you will add \"data sets\" using the new data source.\n1. From within your new project, click \"Data Sources\" \n2. Click \"add data source\"\n3. Enter the following information exactly as shown below then click the \"Create\" button at the lower right of the screen:\n    * Data source name (this can be any text): HDP HDFS\n    * Data source type (dropdown): HDFS - HDP\n    * HDFS host: hdp1.atat.ibm.com\n    * HDFS port:  8020\n    * WebHDFS URL: https://hdp1.atat.ibm.com:8443/gateway/dsx/webhdfs/v1\n    >*When you click \"Create\", the new data source name will be displayed*\n4. Click the name of the new data source to enter the \"View/Edit data source\" screen\n5. Scroll to the bottom of the screen and click \"Add data set\"\n6. Click \"Browse\" - a window opens showing the contents of HDP HDFS, the same content you previewed earlier\n>*If you don't see a list of files open up, check your data source to be sure you correctly entered the settings listed above*\n7. Navigate to \"/data/FinancialServices/Churn/\" in the file tree\n8. For each of CSV files listed in the table in step 1.2 above, do the following\n    * Select the file and select \"Open\" (scroll down)\n    * For \"Remote data set name\" copy and paste the filename but exclude the csv file extension (churn_rate_visualization, cust_summary_notebook_training, cust_summary_visbuilder_training)\n    * Click \"Create\", the scroll down to click \"Add data set\" again, then \"Browse\" and repeat for the remaining files.\n    * When finished, click Save\n9. Verify the new data sets are accessible\n    * Click the name of your project to return to the main project screen\n    * Click \"Assets\" then either click \"Data Sets\" in the navigation bar, or scroll down to the list of Data Sets.\n    * Select \"Preview\" from the 3-dot menu to the right of each of the data sets.  Preview data should be displayed.  Close the preview window.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 3. Create and run the Python customer churn visualization notebook\nIn this step you will use Jupyter and Python to explore and visualize historical customer churn data.  The visualizations will reveal that customer churn is influenced by factors such as income, age, and state of residence.  When using Jupyter notebooks, DSX Local allows programming in the Python, Scala, or R language.  In this lab section, you will create a new Python notebook from a file downloaded earlier, then you will modify it to create datasets from HDP HDFS using the Pandas data analysis library and Brunel visualization library.  As you follow the procedures below, be sure to review the output of each cell as it is executed.\n1. From within your project, either click \"Create notebook\" from the top right + menu, or click \"Add notebook\" from the assets list screen.\n2. Select \"From File\"\n3. At the bottom of the screen click \"browse\", select the __Churn+Visualization+Python+HDP.ipynb__ file (the plus signs were added when you downloaded the file), and click \"Open\".   The \"Name\" field is automatically filled in.  (*You can remove the plus signs from the name if you would like to but it is not necessary*)\n4. Click \"Create\".  A \"Launching Jupyter\" message will be displayed.  __This message may last for a couple minutes, please be patient__\n5. Once the notebook opens, you will see a series of cells.  The notebook has not yet been executed and must be modified before running.  Perform the following steps in order:\n    1. Run the first cell.  To do this, click inside the first cell at the top of the notebook and then click the \"Run\" icon in the toolbar (__>|__).  Within the brackets to the left of the cell, you should see an \\* appear and then change to the number \"1\".  This indicates the cell was run successfully.\n    2. Now click in the cell below, where you will see a \"TODO\" comment.  Click just below the \"TODO\" comment, so the cursor is on the blank line below.  \n    3. Now click the \"*1001*\" \"Find data\" icon above on the right in the title bar.  The \"Find data\" menu opens, click on \"Remote\" and you will see a list of datasets\n    4. Underneath the \"churn_rate_visualization\" data set, click the dropdown arrow and select \"__Insert Pandas DataFrame__\".  *(If you don't see this option then see the note below)* Code to load the dataset from HDP HDFS will automatically be inserted into the cell where the cursor was positioned.  Now close the \"Find data\" menu by clicking the \"X\" on the left side of the menu.  \n    > NOTE:  There is a bug in the application here that you may need to work around.  If you see *Insert Spark DataFrame in R* in the dropdown (instead of the Python options) then you will need to close the notebook by clicking on the project name, stop the notebook's kernel by selecting \"stop kernel\" from the 3-dot menu to the right of the notebook, then reopen the notebook.  **As the notebook is opening, don't click anything until after the kernel has finished starting.** Once the \"Kernel starting, please wait...\" messages have disappeared, restart the lab procedure from step A above (rerun cell 1).\n    5. After the pandas code is automatically inserted, run the cell by clicking the \"Run\" icon in the toolbar (__>|__).  You should see the tabular output of the dataframe.head() statement displayed in the cell output.  \n    6. Click in the next cell down, find the \"TODO\" marker.  Follow the instructions provided within the cell, then run the cell.  You should see a Brunel visualization of the churn rate data.\n    7. Click in the next cell down, find the \"TODO\" marker.  Click on the blank line below the comment and insert the \"customer summary visualization\" dataset as a pandas dataframe by using the \"Find data\" menu as before.  After the pandas code is automatically inserted, run the cell.  You should see a tabular representation of the dataframe.head() statement in the cell output area.\n    8. Click in the next cell down, find the \"TODO\" marker.  Follow the instructions provided within the cell, then run the cell.  Tabular output should show customer mean income grouped by state.\n    9. Click in the next cell down, a markdown cell labeled \"Income by state\".  While this cell is selected, go to the \"Cell\" dropdown menu and select \"Run All Below\" (this runs the selected cell and all cells below it).\n    10.  Scroll down through the notebook to ensure that all remaining visualizations of the customer churn data ran and are displayed.\n6. When you have completed running the notebook successfully, save the notebook by selecting `File -> Save and Checkpoint`.  The message \"Checkpoint created\" will appear in the toolbar (the message will disappear quickly).\n7. Close the notebook by clicking on your project's name.\n8. To conserve resources, before continuing, __Stop the Kernel__ that was started for your notebook by selecting \"Stop Kernel\" from the 3-dot menu to the right of the notebook name.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 4. Create and run the Scala customer churn notebook \nIn this section you will programmatically create a machine learning model using the Scala language that can predict the probability that a given customer will churn.  You will create a new Jupyter Scala notebook from a file downloaded earlier, modify it to use a dataset from HDP HDFS, and incrementally modify and execute the notebook.  This procedure demonstrates using a programmatic approach to train, evaluate, publish, deploy, and test a machine learning model from within a Jupyter notebook.   As you follow the procedures below, be sure to review the output of each cell as it is executed.  \n\n> NOTE: The cells in the sections marked \"optional\" at the end of the notebook should not be executed unless you already have a WML service, WML credentials, and are familiar with the WML service in IBM Cloud.\"\n\n1. From within your project, either click \"Create notebook\" from the top right + menu, or click \"Add notebook\" from the assets list screen.\n2. Select \"From File\"\n3. At the bottom of the screen click \"browse\", select the __Churn ML Training Notebook Scala HDP LR__ file, and click \"Open\".   The \"Name\" field is automatically filled in.  \n4. Click \"Create\".  A \"Launching Jupyter\" message will be displayed.  __This message may last for a couple minutes, please be patient__\n5. Once the notebook opens, you will see a series of cells.  The notebook has not yet been executed and must be modified.  Perform the following steps in order:\n    1. Run the first 3 code cells.  To do this, click inside the first cell at the top of the notebook and then click the \"Run\" icon in the toolbar (__>|__).  Within the brackets to the left of the cell, you should see an \\* appear and then change to the number \"1\".  This indicates the cell was run successfully.  Repeat for the next 2 code cells until you reach the first \"TODO\" marker.\n    > NOTE:  Cell 1 in this notebook should only be run one time (refer to the notes in the notebook).  If you do need to rerun this cell, you will need to restart the notebook's kernel.\n    2. In the cell below the \"Loading Data from HortonWorks Connection\" label, read and follow the \"TODO\" instructions provided within the cell.  Use the \"Find data\" menu as before to insert the remote data set, this time as a Spark DataFrame.  When inserting the dataset code, be sure to click just below the comments, so the cursor is on the blank line below.  __(NOTE:  When renaming \"sc\" to \"scl\" in the getRemoteDataSet function, \"scl\" contains a lowercase \"L\", not a number 1)__  Once you have the code inserted and modified, run the cell.  You should see tabular output from the dataframe.show() statement.\n    > NOTE:  There is a bug in the application here that you may need to work around.  If you see *Insert Spark DataFrame in R* in the dropdown (instead of the Python options) then you will need to close the notebook by clicking on the project name, stop the notebook's kernel by selecting \"stop kernel\" from the 3-dot menu to the right of the notebook, then reopen the notebook.  **As the notebook is opening, don't click anything until after the kernel has finished starting.** Once the \"Kernel starting, please wait...\" messages have disappeared, restart the lab procedure from step A above (rerun cell 1).\n    3. Click in the next cell down, find the \"TODO\" marker, follow the instructions contained within the cell, then run the cell.  You should see tabular output from the churndata.show(5) statement.\n    4. Continue running cells one at a time -- Stop running cells after you execute the cell titled \"Displaying the evaluation results - ROC curve with Brunel\" which renders a Brunel visualization of the model's ROC curve. *The cells you just ran performed several operations:  First the data was split into train/test/validate sets, then a LR model was built and validated, then a visualization of the ROC curve was generated.*\n    5. In the cell below the \"Publish Locally - Use repository service to save model\" label, find the \"TODO\" marker and follow the instructions contained within the cell.  **Be sure to rename both the model and the author values, and ensure the model name is a unique name that you will recognize later.** Run the cell.  **NOTE:  When you run this cell you will see 3 warnings from \"SLF4J\" which you can safely ignore.**\n    6. Click in the next cell down, below the \"Deploy Locally: Create online deployment of published model in DSX Local ML Service\" label and find the \"TODO\" label.  Follow the instructions provided within the cell.  **Be sure to give the model deployment a unique name that you will recognize later.**  After following the instructions at the top of the cell you can run the cell.  You should see output that includes the HttpResponse received from the deployment service.\n    7. Run the next 3 cells to invoke and test the model using the DSX Local Machine Learning service.   *The first cell outputs the REST scoring endpoint for the model, the second cell outputs the fields and values that will be passed to the model, and the third cell outputs the HttpResponse from the model that includes the prediction and probability for churn*\n    > Note:  At this point do not run any more cells.  The remaining cells in the notebook are **optional** and can only be performed if you already have a Bluemix account with a WML service instance and associated WML credentials.\n    \n6. Save the notebook by selecting `File -> Save and Checkpoint`.  The message \"Checkpoint created\" will appear in the toolbar.\n7. Close the notebook by clicking on your project's name.\n8. To conserve resources, before continuing, __Stop the Kernel__ that was started for your notebook by selecting \"Stop Kernel\" from the 3-dot menu to the right of the notebook name.\n9.  The notebook programmatically created and deployed a machine learning model.  Click on your project name, then click on Models in the navigation bar.  You will see the model that was created programmatically.  Click on the model to view its details.  From within the model details screen, scroll down to see the model deployment that was created programmatically.  Click on the model deployment to review its details.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 5. Train, Publish, and Deploy an ML model using the DSX Local Model Builder\n\nIn this lab section, you will use the DSX Local Model Builder GUI to train, publish, and deploy an ML model using a data file from HDP HDFS.  During this process you will train, evaluate, deploy, and test a new machine learning model for predicting the probability that a given customer will churn.  This process is similar to the programmatic approach you just completed using Scala, but this time you will use the DSX Local Model Builder which allows creation of machine learning models without writing code.\n\n1. Click on your project name, click \"Assets\", then either scroll down to \"Models\" or click the \"Models\" link along the top.\n2. Click \"Add model\" next to the __+__ sign\n3. In the \"Name\" field, enter \"*TEAMNAME* Wizard Churn Model\" *(replacing TEAMNAME with your team's name - ateam, bteam, etc) * **Be sure to give the model a unique name that you will recognize later.**\n4. For the \"Method\" selection, click \"__Manual__\", the click \"Create\"\n5. On the \"Select data asset\" screen, click the link for the \"__cust_summary_visbuilder_training__\" remote data set, a preview of the data set will be displayed.  Click \"__Use this data__\".  *(alternatively, you can simply click the radio button next to the data set and click next to skip the data preview)*\n> NOTE:  The Model Builder may display the message \"Loading data\" for up to several minutes, please be patient.  Notify a lab instructor if after several minutes the system is still displaying the \"Loading data\" message.\n6. On the \u201cPrepare data set\u201d screen, note the default selected transformer \u201cAuto Data Preparation\u201d on the right. This is the transformer that we will be using. Click \u201cAdd a transformer\u201d on the top-right of the sceen to note other available transformers. After reviewing the list of transformers, dismiss the dialog and then click Next.\n7. On the \"Select a technique\" screen, do the following:\n    1. In the \"Column value to predict\" dropdown, select the \"CHURN\" column. (The goal of the model is to predict whether or not a customer will churn)\n    2. Click \"Binary Classification\" as the technique.  (It should already be selected as the suggested technique.  )\n    3. Accept the default split shown in the sliders for Train/Test/Holdout \n    4. Click \u201cAdd Estimators\u201d on the top-right of the screen.\n    5. Select \"Logistic Regression\" and click \"Add\"  \n        > Note: Choose **only one** Estimator (Logistic Regression) for this excercise\n    6. Click Next\n8. A \"Training models\" status message will appear - this can take some time, be patient.\n9. When model training has completed, review the metrics for the LR model (such as AREA UNDER ROC CURVE).  The radio button next to \"Logistic Regression\" should already be selected.  Click \"Save\" to save the new model, and click \"Save\" again in the confirmation dialog.  \n    > A \"Saving model\" status message will appear, then you will be returned to the assets list in the project. *Remember the name of the model you just created, you will need it in the following steps.*\n10. DSX Local supports hybrid ML scenarios:  In this step you will review the option to publish a model to the Bluemix WML service. *(We will not actually publish to WML)*\n    * Do this by clicking the 3-dot menu to the right of your new ML model and selecting \"Publish Model\".  Click \"Cancel\" after reviewing the dialog options.\n    > If you were publishing to the WML service, you would paste the username/password credentials (long alphanumeric GUIDs) into this dialog.  The model would be published (saved) to your WML service within the IBM cloud.  \n    \n11. Deploy your new model to the DSX Local ML service  \n    1. Click on your project name, click on \"Assets\", click on \"Models\", and locate the model you just created and saved using Model Builder. \n    1. Select \"Deploy\" from the 3-dot menu to the right of your saved model.  *Be sure to deploy the right model - choose the model you created with Model Builder.*\n    2. In the \"Create Deployment\" dialog box, name your deployment \"__Deployed *TEAMNAME* Churn Model__\" (using your team's name)\n    3. Select \"Online\" from the \"Type\" dropdown menu and click \"Create\".  \n    4. When the deployment is complete, you will be taken to the \"Deployments\" section of the DSX Local Model Management UI.\n     > The model you just created and deployed with the Model Builder is a different model than the one you created and deployed earlier using a programmatic approach in the Scala notebook.\n    \n12. Test your deployed model using the Test API feature in DSX Local\n    1. From the \"Deployments\" list in the Model Management UI, Click the deployment name for the model you created with Model Builder.\n    2. Review the details for the deployment, then click the \"Test API\" button on the far top right of the screen.\n    3. On the Test API screen, keep the the default test values and click \"Predict\".  The model's prediction for this customer to churn appears, along with a pie chart representation of the probabilities.\n    4. Modify the NEGTWEETS value in the \"Input Data\" section (scroll down).  Increase NEGTWEETS to 10 and click \"Predict\" again.  Note how the predicted churn value for this customer has changed.  \n13. When finished, click \"Close\" to exit the Test API screen", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 6. Use Model Management Features to schedule evaluation of deployed ML model\n\nIn this lab section, you will use the model management featues in DSX Local to schedule periodic evaluation of your deployed ML model.  When machine learning models are put into production, DSX model management features provide ongoing evaluation to ensure acceptible performance of the model. \n\n1. From the 3-Horizontal Line menu (aka \"Hamburger\" menu at the top left of DSXL UI), select \"Model Management\"\n2. Click the Deployments link to see all deployments\n3. Click the deployment of the model that __your team__ created with the DSX Model Builder\n4. Scroll to the bottom of the deployment details screen and click \"Schedule Evaluation\"\n5. On the \"Schedule Evaluation\" screen, do the following\n    1. Choose \"BinaryClassiferEvaluator\" from the Evaluator dropdown menu\n    2. Check the \"Use performance metrics to monitor this model\" checkbox\n    3. Keep the the radio button for \"areaUnderROC\" selected and accept the default of .7 for \"Notify when less than\"\n    4. In the \"Schedule\" section, click on the \"Starts at\" selection and slide both sliders (below the calendar) all the way to the left (this will schedule evaluation to take place 10 minutes from the current time)\n    5. Select \"Every Day\" as the Repeat option\n    6. In the \"Remote Data Sets\" section, select \"cust_summary_visbuilder_training\" as the evaluation data set and then click \"__Schedule__\"\n    > Note:  Normally the evaluation data set would contain updated data.  In this case we are evaluating the model using the same data that was used for training.\n6. In 10-12 minutes (shortly after the scheduled evaluation time) you will see the result of the model evaluation displayed in the Model Management UI.  \n    > The completed deployment evaluation should show a green checkmark (indicating success) on the Dashboard tab of the Model Management UI.  The list of all deployment evaluations for a deployed model are visible at the bottom of the deployment details window. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "****\n### *This DSX Local Hands On Lab and associated Skytap environment was created by the IBM Data Science Elite Team*\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "celltoolbar": "Raw Cell Format", 
        "language_info": {
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }
    }, 
    "nbformat_minor": 1
}
