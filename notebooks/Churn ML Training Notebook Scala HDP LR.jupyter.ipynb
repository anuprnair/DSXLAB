{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ML Notebook for Banking Churn Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Importing Brunel and ML Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.3.jar -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//import libraries\n",
    "import org.apache.spark.{SparkConf, SparkContext, SparkFiles}\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession, Row}\n",
    "import org.apache.spark.SparkFiles\n",
    "\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.ibm.transformers.RenameColumn\n",
    "\n",
    "import com.ibm.analytics.ngp.ingest.Sampling\n",
    "import com.ibm.analytics.ngp.util._\n",
    "import com.ibm.analytics.ngp.pipeline.evaluate.{Evaluator,MLProblemType}\n",
    "\n",
    "import com.ibm.analytics.wml.{Learner, Target}\n",
    "import com.ibm.analytics.wml.cads.CADSEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading Data from HortonWorks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// TODO:  Insert \"cust_summary_notebook_training\" remote data set as Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// TODO: Rename the dataframe df1 in the statement below to match the dataframe automatically inserted above\n",
    "val churnDataRaw = df1\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "val toDouble = udf {churn: Boolean => (if(churn) 1.0 else 0.0)}\n",
    "\n",
    "val churnData = churnDataRaw.select(\"AGE\", \"ACTIVITY\", \"EDUCATION\", \"GENDER\", \"STATE\", \"NEGTWEETS\", \"INCOME\", \"CHURN\").\n",
    "                             withColumn(\"label\", toDouble(churnDataRaw.col(\"CHURN\"))).\n",
    "                             drop(\"CHURN\")\n",
    "churnData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val train = 70\n",
    "val test = 15\n",
    "val validate = 15\n",
    "\n",
    "//Split the data into training data set, testing data set, and validation data set\n",
    "\n",
    "val splits = Sampling.trainingSplit(churnData, train, test, validate)\n",
    "\n",
    "val trainingDF = splits._1\n",
    "val testDF = splits._2\n",
    "val validationDF = splits._3\n",
    "\n",
    "println(\"Training data set\")\n",
    "trainingDF.show(5)\n",
    "\n",
    "println(\"Testing data set\")\n",
    "testDF.show(5)\n",
    "\n",
    "println(\"Validation data set\")\n",
    "validationDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building and Evaluating LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//Feature definition\n",
    "\n",
    "val genderIndexer = new StringIndexer().setInputCol(\"GENDER\").setOutputCol(\"gender_code\")\n",
    "val stateIndexer = new StringIndexer().setInputCol(\"STATE\").setOutputCol(\"state_code\")\n",
    "val educationIndexer = new StringIndexer().setInputCol(\"EDUCATION\").setOutputCol(\"education_code\")\n",
    "\n",
    "val featuresAssembler = new VectorAssembler().setInputCols(Array(\"AGE\", \n",
    "                                                         \"ACTIVITY\", \n",
    "                                                         \"education_code\", \n",
    "                                                         \"NEGTWEETS\", \n",
    "                                                         \"INCOME\",\n",
    "                                                         \"gender_code\",\n",
    "                                                         \"state_code\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "//Logistic Regression\n",
    "val lr = new LogisticRegression().setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(genderIndexer, \n",
    "                                              stateIndexer, \n",
    "                                              educationIndexer,\n",
    "                                              featuresAssembler,\n",
    "                                              lr))\n",
    "val newModel = pipeline.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegressionModel}\n",
    "\n",
    "// Extract the summary from the LogisticRegressionModel instance \n",
    "val lrModel = newModel.stages(4).asInstanceOf[LogisticRegressionModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "\n",
    "val testDFWithPredictions = newModel.transform(testDF)\n",
    "val testData = testDFWithPredictions.drop(\"prediction\", \"rawPrediction\", \"probability\")\n",
    "val trainingSummary = lrModel.evaluate(testData)\n",
    "val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "\n",
    "// The ROC curve and area under the ROC curve on test data\n",
    "val rocOnTestData = binarySummary.roc\n",
    "println(s\"Area under ROC curve for the initial model: ${binarySummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Displaying the evaluation results - ROC curve with Brunel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%brunel data('rocOnTestData') x(FPR) y(TPR) line tooltip(#all) axes(x:'False Positive Rate':grid, y:'True Positive Rate':grid) title('ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Save locally:  Save trained model to DSX Local Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// TODO:  Rename values in code below to replace the label TODO_CHANGE_TO_TEAMNAME with your lab team's name \n",
    "\n",
    "// DSX Local Machine Learning - Use ML client to save model.\n",
    "\n",
    "import com.ibm.analytics.ngp.dsxML._\n",
    "import spray.json._\n",
    "\n",
    "val ml_client=ML()\n",
    "//val modelName=\"TODO_CHANGE_TO_TEAMNAME Banking Churn Notebook Model LR\"\n",
    "val modelName=\"TODO_CHANGE_TO_TEAMNAME-BankingChurnNotebookModelLR\"\n",
    "\n",
    "// API specification:  save(model, trainData, testData, metrics, name, description,filename, algorithmType, props: (String,String)*)\n",
    "val saveResult=ml_client.save(newModel,\n",
    "                              trainingDF,\n",
    "                              testDF,\n",
    "                              None,\n",
    "                              modelName,\n",
    "                              \"Prediction for customer to churn from business\",\n",
    "                              \"Churn ML Training Notebook Scala HDP LR.ipynb\",\n",
    "                              \"Classification\")\n",
    "\n",
    "saveResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Locally:  Test model in DSX Local Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import play.api.libs.json._\n",
    "import scalaj.http.{Http,HttpOptions}\n",
    "\n",
    "val json_map=Json.toJson(List(Json.toJson(Map(\n",
    "    \"AGE\"->Json.toJson(23),\n",
    "    \"ACTIVITY\"->Json.toJson(3),\n",
    "    \"EDUCATION\"->Json.toJson(\"Masters degree\"),\n",
    "    \"GENDER\"->Json.toJson(\"M\"),\n",
    "    \"STATE\"->Json.toJson(\"NY\"),\n",
    "    \"NEGTWEETS\"->Json.toJson(7),\n",
    "    \"INCOME\"->Json.toJson(878657)\n",
    "))))\n",
    "\n",
    "val projectName=sys.env(\"DSX_PROJECT_NAME\")\n",
    "val authToken=sys.env(\"DSX_TOKEN\")\n",
    "\n",
    "val scoringURL=s\"http://dsx-scripted-ml-python2-svc.dsxl-ml:7300/api/v1/score/unpublished/${projectName}/${modelName}\"\n",
    "println(scoringURL)\n",
    "\n",
    "val payload_scoring=Json.stringify(json_map)\n",
    "println(payload_scoring)\n",
    "\n",
    "val response_scoring=Http(scoringURL).postData(payload_scoring).header(\"Content-Type\",\"application/json\").header(\"Authorization\",authToken).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\n",
    "response_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Developed/Updated by Alexander Petrov, Matt Walli, Anup Nair Data Science Elite Team, IBM Analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
